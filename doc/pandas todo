OPEN ITEMS
    start breaking the init into subfolders, then clean up import blocks in class files so that they just import
        every class from the folder they're in
        current code works on the E part of the ETL, but we still need a T and L folder.

    FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a
        future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior,
        set `pd.set_option('future.no_silent_downcasting', True)` self.goalie_season_df =
        self.goalie_season_df.fillna(0)

    FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version
        of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.

    FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future
        version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the
        old behavior, exclude the relevant entries before the concat operation.

    UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3
        DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
        swap the mysql db connector for the sqlalchemy db connector and remove the mysql connector's file

    Google says iterrows is slow and that seems to be the case - maybe try factoring it out?

    there's a bunch of cruft in the check* functions for Scheduler.py that should probably be broken out into its own
        set of helper functions - particularly each of the DB calls

	build normalized database schema from the imported data
		* assign import tables to an import database schema
		* build import schema agent, separate from normalized schema agent and access
		reduce each import table to normalized components
			build tables to store these normalized components
			write objects to update and control these normalized components

	DRP - backup code & database plan

	build visualization interfaces
		Tableau - 5 example reports
		PowerBI - 5 example reports

	web blog

	implement SQL abstraction via SQLAlchemy ORM
	    (or other alternatives? isn't there a VRM or something, too?)
	    consider writing the db initialization (table and index creation, mostly)
	    using automap_base() on a connection, we can end up with a Base object full of Table objects
	    try to find a pattern that swaps between pandas DataFrame objects and sqlalchemy Table objects
	        this may mean that i can sometimes skip pandas on my way to the database
	        this may mean i don't need to use pandas to build the controller that converts import tables to
	        normalized tables

********************************************************************************
    * when moving game date and TIMES into normalized tables, reduce time to GMT
        further, resolve datetime() calls in the code base to ask for GMT instead of local
        datetime.now(timezone.utc)

    * the single-triCode override for queryNHL in Seasons.py is pretty kludgy and should be refactored

    * change constructors for all objects
        * remove all but serializing attributes from __init__ (player_id=, game_id=, etc)
        * move attribute initializations to inside the init

	* re-poll everything from the NHL

    * enforce "single query-result" restrictions on all Import objects, such that each parses a single query result.
        * Import objects should be responsible for a single page result
            * refactor all objects as with the most recent changes to the GameCenterImport, and later the PlayerImport
            * create a supporting Controller object
                * Controller object has appropriate ImportLog object(s)
                * Controller object can ask ImportLogs for OpenWork - a list of pages that should be
                * queried one at a time by the controller
                * Controller object can iterate over the OpenWork list, running the associated object's
                * 'queryNHLandUpdateDB' functions
                * Completing an OpenWork list should be the step in process that makes an entry to ImportTableUpdate

    * rename ImportTableUpdateLog.queryDB() to something more appropriate -
        * this method returns last update for all tables

    Player_Import_Log.Py
        * complete
        * test

	Players.Py
        * complete
        test

   Scheduler.Py
	    * add a scheduler object
		* use this object to control all the other objects in order to poll the NHL for updates when appropriate
		    * use the api_query_order_md file to remember the order queries need to happen in
		* schedule update criteria/triggers for each database table
        * knit these update triggers in with the logging objects, and improve logging objects as required
        * test

    Rosters.Py
       * complete
       * test

	Shifts.Py
	    * complete
		* test

    Games_Import_Log.Py
        complete
        test

	Game_Center.Py
	    * start by creating a GameImportLog object, update it from the DB, and then poll it for open work
	    * use https://api-web.nhle.com/v1/gamecenter/2018030417/play-by-play as a modern reference ;)
	    * break into EVEN MORE OBJECTS!! one to control each db table
		* capture play by play, tv broadcasts, rosters, and game results summary info from game center details
		* add single gameId overrides (updateDB, insertDB, queryDB, queryNHL)
		* add ImportTableUpdate logging
		* add GamesImportLog logging
        * test

    Rosters.Py
        * update logging for PlayersImportLog changes

	Players.Py
		* begin transition from function to object code
		* use Game_Center patterns as guide
		* use queries & magic numbers already captured in existing code
		* build out objects for player_bios, player_awards, goalie_career_totals, goalie_season_totals,
		    skater_career_totals, and skater_season_totals
        * test the 6 objects across 4 players - one each of retired/active goalies & skaters

2024-09-13
	GamesImportLog.Py
	    * update table to include newly introduced objects
	    * update create_import_tables file

	GamesImportLog.Py
		* ensure only unique gameIds are inserted, all other actions convert to update
        *    throw a non-existent game_id at games_import_log.queryDB(game_id=[]) and see what happens
        *    what we want is a "no match" response that can be consistently read
        *        on "no match", insert should work as normal
        *        on "yes match", insert should invoke updateDB as an override
		* test

	PlayerImportLog.Py
		* ensure only unique gameIds are inserted, all other actions convert to update
        *    throw a non-existent player_id at player_import_log.queryDB(player_id=[]) and see what happens
        *    what we want is a "no match" response that can be consistently read
        *        on "no match", insert should work as normal
        *        on "yes match", insert should invoke updateDB as an override
		* test

2024-09-11
	Game_Center.Py
	    * use https://api-web.nhle.com/v1/gamecenter/2018030417/play-by-play as a modern reference ;)
	    * was Play_by_Play.Py
		* rename this file Game_Center.Py
		* create new sql table to hold broadcast data
		* recreate the sql table to record game roster spots
		* create new sql table to hold game results summary
		*     i think this json divides up into 9 subtables :(
		*     make 9 subtables (it was eight)

2024-09-10
	Games.Py was Schedules.Py
		* add logging
		* rename this as the Games.Py file
		* remove stubbed logging columns and supporting code
		* add single season & single team overrides (updateDB, queryDB, query)
		* add code for GamesImportLog
		* queue up test db connections & test data
		* test new update function (including games_import_log functions)

2024-09-09
	PlayerImportLog.Py
		* rename existing update method to insert
		* add update method
		* test

	GamesImportLog.Py
		* rename existing update method to insert
		* add update method
		* test

2024-08-30
	Rosters.Py
		* add single triCode overrides (updateDB, queryDB, queryNHL)
		* add logging

	Seasons.Py
		* add single triCode overrides (updateDB, queryDB, queryNHL)
		* add logging

	add a ImportTableUpdateLog.py file
		* add ImportTableUpdateLog object to control table of same name
		* add table_update_log table to log when each import table is updated

	Teams.Py
		* add single triCode overrides (updateDB, queryDB, queryNHL)
		* add logging

	add a GamesImportLog.Py file
		* write a GamesImportLog object to control table of same name
		* add table to log when each gameId is checked for its play by play, results, shifts, and roster details

	add a PlayerImportLog.Py file
		* write a PlayerImportLog object to control table of same name
		* add table to log when each playerId is checked for their season, career, and award summaries

2024-08-21
	refactor existing collection & import processes
		break complex logic into more functions
		arrange broader metaphors (teams, games, players, etc) into objects
			assign existing functions to objects
			flesh out objects with missing or needed CRUD elements
			separate "create new table" and "update existing table" processes
			start with games_import - updates for this one is probably most immediately useful
			focus on what happens when playoff schedules emerge & update as playoffs go along

2024-08-16
	refactor existing collection & import processes
		unify naming conventions
			remove all nhl_pandas_ and nhlpandas_ prefixes and import "as nhlpd."
			* rationalize '_import' table names
			push '_import' tables into their own schema
			name objects and functions accordingly
		make object files into a single (easily imported) library
			* build an lib folder for the collection object files
			* build an __init__.py
			* import object files to this __init__ file
			* move main.py and scrap.py out of this folder

2024-08-15
	shift charts
		https://api.nhle.com/stats/rest/en/shiftcharts?cayenneExp=gameId%3E=2023021082%20and%20gameId%3C=2023021094
	store to shifts_import table

2024-08-14
	find brody's missing goal

	improve usage of json_normalize across all pages
		* play_by_play.py
		* players.py
		* rosters.py
		* schedules.py
		* teams.py

2024-05-14
	Build delta process for games scheduled but not played
		identify tables that will need to be appended to
			game_play_by_play_import
			game_rosters_import
			games_import - update only
			shift_charts_import
		check if polling columns are adequate, add tables or columns where appropriate

2024-05-06
	TOI data type conversion problem
		identify fields in existing _import tables that will need support
			goalie_career_totals_import
				regularSeason.timeOnIce
				playoffs.timeOnIce
			goalie_season_import
				timeOnIce
		google standard patterns to solve for this issue
			for MySQL table use DATETIME datatype where time starts at '1000-01-01 00:00:00'
			in Pandas, we'll want to add time values to this start date ('1000-01-01 00:00:00') and then record the date
		commit required table changes

	Re-poll data

2024-03-25
	* get a scrap.py running for SQLAlchemy
		consider using their query framework? - big benefit is it allows others to adopt code without committing to mysql db
	* populate code for TOI data type conversion

2024-03-21
	* start git repo
		daily/hourly commit practice
		publish to public?

2024-03-20
	* clean up last night's python code, clear out test lines and notes cruft
	* clean up docs files
	* update table creation file
	* document python functions as written
	* refactor sql tables with '_import' suffix

2024-03-19
	* build player tables
		* bio/header
		* career
			* goalie
			* player
		* season
			* goalie
			* player
		* awards
		* check log
	* build import functions for each of these tables
	* wire import functions into the rest of players.py functions
	* test completed players.py
